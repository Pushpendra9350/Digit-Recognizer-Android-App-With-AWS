{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubX5fGNqZKiP"
   },
   "outputs": [],
   "source": [
    "# Plot ad hoc mnist instances\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "# load (downloaded if needed) the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# plot 6 images\n",
    "plt.subplot(321)\n",
    "plt.imshow(x_train[0], cmap=plt.cm.binary)\n",
    "plt.subplot(322)\n",
    "plt.imshow(x_train[1], cmap=plt.cm.binary)\n",
    "plt.subplot(323)\n",
    "plt.imshow(x_train[2], cmap=plt.cm.binary)\n",
    "plt.subplot(324)\n",
    "plt.imshow(x_train[3], cmap=plt.cm.binary)\n",
    "plt.subplot(325)\n",
    "plt.imshow(x_train[4], cmap=plt.cm.binary)\n",
    "plt.subplot(326)\n",
    "plt.imshow(x_train[5], cmap=plt.cm.binary)\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TvvmtRuPsset"
   },
   "outputs": [],
   "source": [
    "print(\"shape of x_train data: \", x_train.shape)\n",
    "print(\"shape of y_train data: \", y_train.shape)\n",
    "print(\"shape of x_test data: \", x_test.shape)\n",
    "print(\"shape of y_test data: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOpTumsEHxm2"
   },
   "source": [
    "#First we are going to create simple MLP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwXjiR7IMIJy"
   },
   "source": [
    "## 1. Data Prepration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DHQSg7jTH5sY"
   },
   "outputs": [],
   "source": [
    "# Some important libraries and APIs \n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Because we have data in 3 dimesion (n,28,28) so we have convert it into (n,784) for MLP\n",
    "# This is called flattening \n",
    "flat_pixels = x_train.shape[1] * x_train.shape[2] # 28*28\n",
    "x_train = x_train.reshape((x_train.shape[0], flat_pixels)).astype('float32')\n",
    "x_test = x_test.reshape((x_test.shape[0], flat_pixels)).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DadXEqCnM8Xa"
   },
   "outputs": [],
   "source": [
    "flat_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "egGQHrkNIquy"
   },
   "outputs": [],
   "source": [
    "print(\"shape of x_train data: \", x_train.shape)\n",
    "print(\"shape of x_test data: \", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IN3HcoBnJH-M"
   },
   "outputs": [],
   "source": [
    "# In our dataset we have pixels in the range of 0-255 so we have convert it into 0-1 for simple calcuation\n",
    "# This is called normalization\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5FYalaWLAgC"
   },
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OH3sNVUVJZmV"
   },
   "outputs": [],
   "source": [
    "# As we know we have 0-9 digits to classifiy \n",
    "# Means we have low categories so we can use one hot encoding here for output\n",
    "# np_utils is a numpy utility where we have function called to_categorical with this we do onehot encoding\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aL-H3ERbMxrL"
   },
   "outputs": [],
   "source": [
    "classes = y_test.shape[1]\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UBTCJV6SLHx5"
   },
   "outputs": [],
   "source": [
    "print(\"Now shape is like this 2D shape: \",y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7wE6trPMPf1"
   },
   "source": [
    "## 2. Defining a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yd77ubAxLKJD"
   },
   "outputs": [],
   "source": [
    "# MLP model with 1 hidden layer function \n",
    "# We are using fucntion so that we can change it easily later \n",
    "def MLP_h1_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "    # Dense laye configuration\n",
    "    # First arguments is shape of the output we will get from this layer\n",
    "    # input_dim = size of the input we are providing\n",
    "    # Kernel_initializer = Weight initializer for initial layer\n",
    "    # Activation is relu here to find output for next layer\n",
    "    # flat_pixels=784\n",
    "\tmodel.add(Dense(flat_pixels, input_dim=flat_pixels, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(classes, kernel_initializer='normal', activation='softmax'))\n",
    "\t# Compile model\n",
    "    # https://machinelearningmastery.com/cross-entropy-for-machine-learning/ for more idea\n",
    "    # Optimizer we are using adam and accuracy as a matrix\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OEI5TadWPz4a"
   },
   "outputs": [],
   "source": [
    "model = MLP_h1_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4ZsBLYqP51n"
   },
   "outputs": [],
   "source": [
    "# first two are input and output train ndarrays\n",
    "# Validation_data = how do you want to validate your results on each epochs\n",
    "# Batch size is to tell that how many inputs it needs to take on one updation of weights \n",
    "# Verbose is to how you the progress bas 0,1,2. \n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=200)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"MLP_h1_model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "print(\"MLP_h1_model Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80ADsGTzSmyv"
   },
   "source": [
    "**Observations**\n",
    "As we can see we get an 98.36% accuracy then how we can improve it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-kgAsvrSzkY"
   },
   "source": [
    "# Let's work with CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_J_iydi2R5R8"
   },
   "outputs": [],
   "source": [
    "# Important libraries and Apis for CNN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "# load (downloaded if needed) the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94ecdZHgUYsh"
   },
   "source": [
    "## 1. Data Prepration\n",
    "As we know Convolutional Neural network is working on the Matrices\n",
    "* first, we have to conver our data into acceptable form \n",
    "* What the shape of acceptance of the CNN layer : [pixels][width][height][channels]\n",
    "* so we have convert it into this shape now we have [pixels][width][height]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZDpbTJdT2cx"
   },
   "outputs": [],
   "source": [
    "# to laod data (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# reshape to be [samples][width][height][channels]\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pxbwR1SSVUcu"
   },
   "outputs": [],
   "source": [
    "print(\"New shape is: \",x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "99J92hU9VZV4"
   },
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kkQ5Ne1IM-Y_"
   },
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(len(x_train),28,28,1)\n",
    "x_test = x_test.reshape(len(x_test),28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMoWtBHqbDe7"
   },
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bNDWU7T1VjCE"
   },
   "outputs": [],
   "source": [
    "# simple model with one conv2D layer only \n",
    "def CNN_1L_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "    # https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/\n",
    "    # https://keras.io/api/layers/convolution_layers/convolution2d/ to know more\n",
    "    # 32 is number of filers/kernels we are using in this layer \n",
    "    # (5,5) is the kernel size we are using here \n",
    "    # input is to tell that get each image one by one \n",
    "\tmodel.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "    # 2*2 maxpooling \n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # To reduce 20% data \n",
    "\tmodel.add(Dropout(0.2))\n",
    "    # flatten will make 28*28 to 784\n",
    "\tmodel.add(Flatten())\n",
    "    # These are the dense layer same as MLP\n",
    "\tmodel.add(Dense(128, activation='relu'))\n",
    "    # Last layer with softmax activation\n",
    "\tmodel.add(Dense(classes, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cE0pAbY6tgV6"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYakh4vuasN2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TbRhE68xaylF"
   },
   "outputs": [],
   "source": [
    "def CNN_2L_model():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(30, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Conv2D(15, (3, 3), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu'))\n",
    "\tmodel.add(Dense(50, activation='relu'))\n",
    "\tmodel.add(Dense(classes, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gua9oQZ9eqmM"
   },
   "outputs": [],
   "source": [
    "def CNN_3L1_model():\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\tmodel.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "\tmodel.add(Dense(10, activation='softmax'))\n",
    "\t# compile model\n",
    "\t#opt = keras.optimizers.Adam(lr=0.01, momentum=0.9)\n",
    "\tmodel.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nbBPQHVs8H5"
   },
   "outputs": [],
   "source": [
    "def CNN_3L2_model():\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\tmodel.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D((2, 2)))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "\tmodel.add(Dense(10, activation='softmax'))\n",
    "\t# compile model\n",
    "\t#opt = keras.optimizers.Adam(lr=0.01, momentum=0.9)\n",
    "\tmodel.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2vM7PXIvDJB"
   },
   "outputs": [],
   "source": [
    "def CNN_4L_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    # model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    # model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\t# compile model\n",
    "\t#opt = keras.optimizers.Adam(lr=0.01, momentum=0.9)\n",
    "    model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LnSv9s1MvDF5"
   },
   "outputs": [],
   "source": [
    "model1 = CNN_1L_model()\n",
    "model2 = CNN_2L_model()\n",
    "model31 = CNN_3L1_model()\n",
    "model32 = CNN_3L2_model()\n",
    "model4 = CNN_4L_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KkzBwmPbHhYZ"
   },
   "outputs": [],
   "source": [
    "models = [model1,model2,model31,model32,model4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "crLYFUVtOsh4"
   },
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6MJ7ZlfmJaTS"
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "for model in models:\n",
    "    count+=1\n",
    "    # Fit the model\n",
    "    tb_callback = tf.keras.callbacks.TensorBoard(log_dir = \"/content/logs/layer\"+str(count),histogram_freq = 1)\n",
    "                                                 \n",
    "    model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=200,verbose=0,callbacks=[tb_callback])\n",
    "    # Final evaluation of the m\n",
    "    scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(\"\\nAccuracy: %.2f%%\" % (scores[1]*100))\n",
    "    print(\"Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q28EywaPJaQ4"
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "for model in models:\n",
    "    count+=1\n",
    "    # Fit the model\n",
    "    tb_callback = tf.keras.callbacks.TensorBoard(log_dir = \"/content/logs1/layer\"+str(count),histogram_freq = 1)\n",
    "                                                 \n",
    "    model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=200,verbose=0,callbacks=[tb_callback])\n",
    "    # Final evaluation of the m\n",
    "    scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(\"\\nAccuracy: %.2f%%\" % (scores[1]*100))\n",
    "    print(\"Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61hjbGGlJaM-"
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "for model in models:\n",
    "    count+=1\n",
    "    # Fit the model\n",
    "    tb_callback = tf.keras.callbacks.TensorBoard(log_dir = \"/content/logs3/layer\"+str(count),histogram_freq = 1)\n",
    "                                                 \n",
    "    model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=200,verbose=0,callbacks=[tb_callback])\n",
    "    # Final evaluation of the m\n",
    "    scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(\"\\nAccuracy: %.2f%%\" % (scores[1]*100))\n",
    "    print(\"Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3_CglROJaEa"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qH_0dA4LUJ9f"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir /content/logs1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ukry3n0eZBY7"
   },
   "source": [
    "**Observations**<br>\n",
    "After 5 time running 5 models we get our second model is doing we "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfQ5bzqrfDsQ"
   },
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANCdDTM-fFyJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ResearchOnCNN.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
